import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

#importing standard libraries
from sklearn import preprocessing 
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn import linear_model

import matplotlib.pyplot as plt
import os

filepath = '../input/bike-sharing-dataset/day.csv'
bikedata=pd.read_csv(filepath,index_col=0)

#checking for null values
null_value=bikedata.isnull()
null_value.head()

#checking for unique values
bikedata['yr'].unique()

bikedata.describe()

#dropping useless columns
columnsToDrop=['instant','casual','registered','atemp','dteday']
bikedata=bikedata.drop(columnsToDrop,axis=0)
bikedata.head()

#feature scaling the temp and windspeed column
columntoscale=['temp','hum','windspeed']
scaler=StandardScaler()
bikedata[columntoscale]=scaler.fit_transform(bikedata[columntoscale])
bikedata[columntoscale].describe()

bikedata['daycount'] = pd.Series(range(bikedata.shape[0]))/24

from sklearn.model_selection import train_test_split
train_set, test_set=train_test_split(bikedata, test_size=0.3, random_state=42)
train_set.sort_values('daycount',axis=0, inplace=True)
test_set.sort_values('daycount',axis=0, inplace=True)

print(len(train_set), "train +", len(test_set), "test")

#defining function
def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())
    
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost
from xgboost import XGBRegressor

trainingCols = train_set.drop(['cnt'],axis=0)
trainingLabels = train_set['cnt']

#Train a Decision Tree Regressor
dec_reg = DecisionTreeRegressor(random_state = 42)
dt_mae_scores = -cross_val_score(dec_reg, trainingCols, trainingLabels, cv=10, scoring="neg_mean_absolute_error")
display_scores(dt_mae_scores)
dt_mae_scores = np.sqrt(-cross_val_score(dec_reg, trainingCols, trainingLabels, cv=10, scoring="neg_mean_absolute_error"))
display_scores(dt_mae_scores)

#training random forest regressor
forest_reg = RandomForestRegressor(n_estimators=150, random_state=42)
rf_mae_scores = -cross_val_score(forest_reg, trainingCols, trainingLabels, cv=10, scoring="neg_mean_absolute_error")
print(rf_mae_scores)
display_scores(rf_mae_scores
rf_mse_scores = np.sqrt(-cross_val_score(forest_reg, trainingCols, trainingLabels, cv=10, scoring="neg_mean_absolute_error"))
print(rf_mse_scores)
display_scores(rf_mse_scores)

from sklearn.model_selection import GridSearchCV
#defining variable
param_grid = [
    {"n_estimators": [120,150], 'max_features': [10,12], 'max_depth': [15,28]},
]
grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')

grid_search.fit(trainingCols, trainingLabels)

print(grid_search.best_estimator_)
print(grid_search.best_params_)

#evaluation of model
final_model = grid_search.best_estimator_
test_set.sort_values('daycount',axis=0,inplace=True)
test_x_cols = (test_set.drop(['cnt'], axis=1)).columns.values
test_y_cols = 'cnt'

X_test = test_set.loc[:,test_x_cols]
y_test = test_set.loc[:,test_y_cols]

#evaluation of model on test data and calculate root mean square error
test_set.loc[:,'predictedCounts_test'] = final_model.predict(X_test)

mse = mean_squared_error(y_test, test_set.loc[:,'predictedCounts_test'])
final_mse = np.sqrt(mse)
print(final_mse)
test_set.describe()
